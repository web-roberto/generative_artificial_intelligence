{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Reinforcement Learning from Human Feedback con PPO sobre TinyLLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso pr谩ctico</h2>\n",
        "\n",
        "En este caso pr谩ctico, se propone al alumno la realizaci贸n de Reinforcement Learning from Human Feedback para evitar la generaci贸n de contenido t贸xico sobre una versi贸n reducida de LLAMA denominada [TinyLLAMA](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.3)\n",
        "\n",
        "Por oto lado, como algoritmo de recompensa (Reward model) se propone el uso de una versi贸n de [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) con fine-tuning para la detecci贸n de comportamiento t贸xico/hate: https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resoluci贸n del caso pr谩ctico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V8dgd_BUeOK"
      },
      "source": [
        "## 0. Instalaci贸n de librer铆as externas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl xformers trl evaluate sentencepiece"
      ],
      "metadata": {
        "id": "vPIR50tPH6rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNFCLMHWCXs"
      },
      "source": [
        "## 1. Lectura del modelo y del tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Descarga del modelo y del tokenizador"
      ],
      "metadata": {
        "id": "KalsBY6woqmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para reducir el consumo de recursos copmutacionales, sobre todo memoria RAM, durante el proceso de re-entrenamiento y Reinforcement Learning vamos a aplir QLoRA sobre el modelo."
      ],
      "metadata": {
        "id": "SnFaFP9goGzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdK3d284lxFw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Definimos los param茅tros para bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNfU5_BKOTp1"
      },
      "outputs": [],
      "source": [
        "# Nombre del modelo\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "\n",
        "# Leemos el modelo pre-entrenado el modelo LLAMA2-7b-chat\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    low_cpu_mem_usage=True # Reduccion del consumo de cpu y memoria al leer el modelo\n",
        ")\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAl7HAdNPjDT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652jxcqawjQk"
      },
      "source": [
        "### 1.2. Generaci贸n de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ErX-b24n4Ll"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Creamos un pipeline para la tokenizaci贸n y generaci贸n del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCvtETXWJJT"
      },
      "outputs": [],
      "source": [
        "prompt = \"Act煤a como si fueses el mayor experto en historia del mundo. Describe \\\n",
        "en pocas palabras lo que ocurri贸 en la segunda guerra mundial.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "U0gJLc0DJo79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pelMVjGATwUO"
      },
      "outputs": [],
      "source": [
        "# Invocamos el pipeline para realizar generaci贸n de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01t6CnUB_Lns"
      },
      "source": [
        "## 2. Selecci贸n y preparaci贸n del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este caso pr谩ctico vamos a utilizar un conjunto de datos denominado [Dialogsum](https://huggingface.co/datasets/knkarthick/dialogsum):\n",
        "\n",
        "DialogSum es un conjunto de datos de resumen de di谩logos a gran escala, compuesto por 13.460 di谩logos divididos en entrenamiento, prueba y validaci贸n.\n",
        "\n",
        "Ejemplo del conjunto de datos:\n",
        "\n",
        "```\n",
        "{'id': 'train_0', 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'topic': \"get a check-up}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fJqqeQ3gv-aa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBM_wgklBoJE"
      },
      "source": [
        "### 2.1. Lectura del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"knkarthick/dialogsum\")"
      ],
      "metadata": {
        "id": "Y44NC6D8sz2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "TWAT_fRtxUcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducimos el conjunto de datos\n",
        "NUM_EJ_TRAIN = 1000\n",
        "NUM_EJ_VAL = 100\n",
        "NUM_EJ_TEST = 100\n",
        "\n",
        "# Subconjunto de entrenamiento\n",
        "ds['train'] = ds['train'].select(range(NUM_EJ_TRAIN))\n",
        "\n",
        "# Subconjunto de validaci贸n\n",
        "ds['validation'] = ds['validation'].select(range(NUM_EJ_VAL))\n",
        "\n",
        "# Subconjunto de pruebas\n",
        "ds['test'] = ds['test'].select(range(NUM_EJ_TEST))"
      ],
      "metadata": {
        "id": "2uKsPh3o8zJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds['train']['dialogue'][2])"
      ],
      "metadata": {
        "id": "9Afvjeve9A8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Preparaci贸n del conjunto de datos para proporcionarlo al algoritmo"
      ],
      "metadata": {
        "id": "I_XW2E-KCp87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataset(dataset, tokenizer, input_min_text_length, input_max_text_length):\n",
        "\n",
        "    # Filtramos los dialogos que se encuentran entre el tama帽o minimo y maximo\n",
        "    dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"validation\"] = dataset[\"validation\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "    dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        # Plantilla de entrenamiento para cada ejemplo\n",
        "        prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{sample[\"dialogue\"]}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
        "        # Esto debe llamarse \"query\", es un requisito de la biblioteca PPO\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    # Tokenizamos cada dialogo\n",
        "    dataset = dataset.map(tokenize, batched=False)\n",
        "\n",
        "    # Convertimos el conjunto de datos a un formato adecuado\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "KYGeGTaCtCbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = prep_dataset(ds, tokenizer, input_min_text_length=200, input_max_text_length=1024)"
      ],
      "metadata": {
        "id": "k-Sg2zOgy95t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[\"train\"][\"query\"][0])"
      ],
      "metadata": {
        "id": "uiEhhyf-tO-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVyogGn0ruV"
      },
      "source": [
        "## 3. Configuraci贸n Reinforcement Learning from Human Feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKOiXMJBMBPD"
      },
      "source": [
        "### 3.1. Configuraci贸n de LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnDnMYV-HYtW"
      },
      "source": [
        "La siguiente funci贸n es interesante para comparar el n煤mero de par谩metros entrenables que tiene el modelo antes y despu茅s de apalicar LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWyRL5IhUpu3"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ7xJDTFUqtF"
      },
      "outputs": [],
      "source": [
        "print(print_trainable_parameters(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxiTkz4zy8aK"
      },
      "source": [
        "Configuramos LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-il4eI-L4yg"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "\n",
        "# Definici贸n de la configuraci贸n de LoRA\n",
        "lora_config = LoraConfig(\n",
        "                 r = 16, # Dimensi贸n de las matrices\n",
        "                 lora_alpha = 16, # LoRA scaling factor\n",
        "                 lora_dropout = 0.05, # Regularizaci贸n\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\" # Tipo de tarea/modelo al que aplicarlo\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtqgSmk7zB5U"
      },
      "outputs": [],
      "source": [
        "# Aplicamos la configuraci贸n al modelo\n",
        "model_peft = get_peft_model(model, lora_config)\n",
        "\n",
        "# Mostramos el n煤mero de par谩metros que se van a entrenar\n",
        "model_peft.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTs5vb_RTQp"
      },
      "source": [
        "### 3.2. Configuraci贸n (Proximal Policy Optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante el proceso de PPO, s贸lo se actualizar谩n algunos par谩metros. En concreto, los par谩metros entrenables con LoRA junto con algunos par谩metros adicionales. Puedes encontrar m谩s informaci贸n sobre esta clase de modelos en [su documentaci贸n](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model).\n",
        "\n",
        "El n煤mero de par谩metros entrenables puede calcularse como `(+1)`\n",
        " donde `` es el n煤mero de unidades de entrada (aqu铆 `=2048`) y `` es el n煤mero de unidades de salida (aqu铆 `=1`). El t茅rmino `+1` en la ecuaci贸n tiene en cuenta el t茅rmino bias.\n",
        "\n",
        "E nuestro caso, el n煤mero de par谩metros entrenables debe ser: `2,252,800 + 2.049 = 2.254.849 par谩metros`"
      ],
      "metadata": {
        "id": "05LCMPE9NAvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "\n",
        "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_peft,\n",
        "                                                              torch_dtype=torch.bfloat16,\n",
        "                                                              is_trainable=True,\n",
        "                                                              device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "print(f'Parametros entrenables PPO Model:\\n{print_trainable_parameters(ppo_model)}\\n')\n",
        "print(ppo_model.v_head)"
      ],
      "metadata": {
        "id": "tHGIu3lpMkYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tal y como hemos comentado en secciones anteriores, adem谩s del modelo que vamos a ir ajustando en el proceso de Reinforcement Learning, se requiere una instancia del mismo modelo con los par谩metros congelados para que sirva de referencia y calcular las probabilidades relativas de los tokens generados.\n",
        "\n",
        "El modelo de referencia representar谩 el LLM antes de la \"desintoxicaci贸n\". Ninguno de los par谩metros del modelo de referencia se actualizar谩 durante el entrenamiento utilizando PPO."
      ],
      "metadata": {
        "id": "R4UYubHiOHrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import create_reference_model\n",
        "\n",
        "ref_model = create_reference_model(ppo_model)\n",
        "\n",
        "print(f'Par谩metros entrenables modelo de referencia:\\n{print_trainable_parameters(ref_model)}\\n')"
      ],
      "metadata": {
        "id": "ecK3Db5JOISo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Creaci贸n del Reward Model"
      ],
      "metadata": {
        "id": "BTKiMGenO5rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo siguiente que debemos hacer es selccionar el modelo de reocmpensas (Reward model).\n",
        "\n",
        "Para este caso pr谩ctico vamos a hacer uso de una versi贸n de [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) con fine-tuning que ha creado Meta (Facebook) para la detecci贸n de comportamiento t贸xico/hate: https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target\n",
        "\n",
        "El modelo predecir谩 las probabilidades de que un texto pertenezca a una de las dos clases: `(no_hate, hate)`"
      ],
      "metadata": {
        "id": "V6EUWaUwO-ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "\n",
        "# Cargamos el modelo\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Cargamos el tokenizador\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    reward_model_name, device_map=\"auto\")\n",
        "\n",
        "# Etiquetas del modelo\n",
        "print(f\"\\nEtiquetas del modelo: {reward_model.config.id2label}\")"
      ],
      "metadata": {
        "id": "xOV5z98sPAvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci贸n se muestra como funcionar铆a el proceso de generaci贸n de la recompensa."
      ],
      "metadata": {
        "id": "KMmyhGgJ3O1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_evaluation(text):\n",
        "\n",
        "  toxicity_input_ids = reward_tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  logits = reward_model(input_ids=toxicity_input_ids.to('cuda')).logits\n",
        "  print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "\n",
        "  # Mostramos las probabilidades para cada categoria: [not hate, hate]\n",
        "  probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "  print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "\n",
        "  # Mostramos la recompensa\n",
        "  not_hate_index = 0\n",
        "  nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "  print(f'reward (high): {nothate_reward}')"
      ],
      "metadata": {
        "id": "SNwT1w5fPloG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Juan que no ha visto la pelicula.\n",
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "\n",
        "reward_evaluation(non_toxic_text)"
      ],
      "metadata": {
        "id": "T8BuZzBARbnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Persona 1# le dice a Tommy que la pel铆cula era terrible, tonta y est煤pida.\n",
        "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "\n",
        "reward_evaluation(toxic_text)"
      ],
      "metadata": {
        "id": "7gNNtMFvRlbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVY45QhS8G4W"
      },
      "source": [
        "## 4. Aplicaci贸n del Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Lectura del conjunto de datos"
      ],
      "metadata": {
        "id": "V8p9KrQ84pn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la lectura de los datos por parte del POO, necesitamos definir un data collator que transforme el formato original en un formato espec铆fico"
      ],
      "metadata": {
        "id": "Gn8umtN-4x-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "9WReXUQpUxsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ],
      "metadata": {
        "id": "hclZuUFQ48S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Configuraci贸n de los par谩metros para el entrenamiento"
      ],
      "metadata": {
        "id": "I6MIrpqMU-dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import PPOConfig, PPOTrainer\n",
        "\n",
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=2\n",
        "batch_size=2\n",
        "\n",
        "config = PPOConfig(\n",
        "    # model_name=model_peft,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=ds[\"train\"],\n",
        "                         data_collator=collator)"
      ],
      "metadata": {
        "id": "GEaNGN32U5VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Reinforcement Learning (Fine-tuning)"
      ],
      "metadata": {
        "id": "AnrvVzHdV32-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este punto vamos a entrar en un bucle en el que se ir谩n actualizando los valores de los par谩metros del modelo utilizando PPO.\n",
        "\n",
        "El bucle consiste en los siguientes pasos principales:\n",
        "\n",
        "1.   Obtener los completions de LLM que se esta ajustando (modelo PEFT).\n",
        "2.   Obtener los sentimientos para las respuestas del modelo utilizando RoBERTa\n",
        "3.   Optimizar el valor de los par谩metros del LLM con PPO utilizando el tr铆o (consulta, respuesta, recompensa).\n",
        "\n",
        "La operaci贸n se est谩 ejecutando correctamente si ves aparecer las siguientes m茅tricas:\n",
        "\n",
        "* `objective/kl`: Este valor se refiere a la divergencia de Kullback-Leibler (KL) entre las distribuciones de probabilidad del modelo re-entrenado y el modelo de referencia. Una divergencia KL baja sugiere que las actualizaciones de los par谩metros no est谩n cambiando dr谩sticamente la pol铆tica, lo cual es generalmente bueno para la estabilidad del entrenamiento.\n",
        "* `ppo/returns/mean`: Este valor representa la recompensa promedio que el agente est谩 obteniendo. En el aprendizaje por refuerzo, el objetivo es generalmente maximizar la recompensa total, por lo que queremos ver este n煤mero aumentar a lo largo del tiempo.\n",
        "* `ppo/policy/advantages_mean`: Este valor se refiere a la funci贸n de ventaja, que mide cu谩nto mejor (o peor) es tomar una acci贸n espec铆fica en un estado espec铆fico, en comparaci贸n con la acci贸n promedio en ese estado. Un valor de ventaja positivo sugiere que la acci贸n es mejor que el promedio, y un valor negativo sugiere que es peor. Al maximizar la funci贸n de ventaja promedio, el algoritmo busca mejorar la pol铆tica para obtener mejores recompensas."
      ],
      "metadata": {
        "id": "IvmmhG7QV6uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          tokenizer=reward_tokenizer,\n",
        "                          model=reward_model_name,\n",
        "                          device=0) # GPU\n",
        "\n",
        "# Argumentos proporcionados para la produci贸n de la recompensa\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
        "    \"batch_size\": 2,\n",
        "    \"padding\":'max_length',\n",
        "    \"truncation\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "vX5U_hEN9Dq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_pipe(non_toxic_text, **reward_kwargs))"
      ],
      "metadata": {
        "id": "eeEffEeM9EXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.core import LengthSampler\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "output_min_length = 100\n",
        "output_max_length = 300\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "# Argumentos proporcionados para la generaci贸n\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "}\n",
        "\n",
        "# N煤mero de iteraciones durante el prceso de RL\n",
        "max_ppo_steps = 15\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Terminamos el bucle cuando alcanzamos el m谩ximo de iteraciones\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    print(f\"\\nIteraci贸n {step} del proceso de Reinforcement Learning...\")\n",
        "    # Leemos los prompts de entrada para realizar la generaci贸n\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Generamos las completions del LLM (TinyLLAMA)\n",
        "    summary_tensors = []\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        print(\"Procesando prompt...\")\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # Destokenizamos los completions. Este campo debe llamarse \"response\"\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    # Mostramos por pantalla las completions\n",
        "    print(f\"Completions: {batch['response']}\\n\")\n",
        "\n",
        "    # Calculamos la recompensa para los completions generados\n",
        "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "\n",
        "    # Calculamos la recompensa a partir del valor \"not_hate\"\n",
        "    not_hate_index = 0\n",
        "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "\n",
        "    # Ejecutamos un paso de optimizaci贸n de los par谩metros de TinyLLAMA con PPO\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'\\nobjective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "id": "xgPkknJsVm5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Guardamos el modelo en disco"
      ],
      "metadata": {
        "id": "l9n_oOmkICuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el modelo en disco\n",
        "ppo_model.save_pretrained(\"/content/drive/MyDrive/TinyLLAMA-ppo\")"
      ],
      "metadata": {
        "id": "NyNJ3F38HAWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYmLeJXAH6k_"
      },
      "source": [
        "## 5. Generaci贸n de texto con TinyLLAMA con RLHF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo del conjunto de pruebas\n",
        "print(ds[\"test\"][\"dialogue\"][10])"
      ],
      "metadata": {
        "id": "xw6eSFChD_Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos aseguramos de que el modelo esta en la GPU\n",
        "ppo_model = ppo_model.to('cuda')\n",
        "\n",
        "# Nos aseguramos de que el tensor de entrada esta en el formato correcto\n",
        "input_ids = torch.as_tensor(ds['test']['input_ids'][10], dtype=torch.long).unsqueeze(dim=0).to('cuda')\n",
        "\n",
        "# Argumentos proporcionados para la generaci贸n\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"input_ids\": input_ids\n",
        "}\n",
        "\n",
        "# Generamos la predicci贸n\n",
        "summary = ppo_model.generate(**generation_kwargs)\n",
        "\n",
        "# Decodificamos la predicci贸n\n",
        "print(tokenizer.decode(summary.squeeze()))"
      ],
      "metadata": {
        "id": "vXCeuDFzjOrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Leemos el modelo de disco"
      ],
      "metadata": {
        "id": "iFS2KgifG610"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBGVT81_07RG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "adapters_name = \"/content/drive/MyDrive/TinyLLAMA-ppo\"\n",
        "\n",
        "print(f\"Cargando el modelo: '{model_name}' en memoria...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\"El modelo: '{model_name}' ha sido cargado correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "fkugO4H8HlzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "CHAT_EOS_TOKEN_ID = 32002\n",
        "\n",
        "# Creamos un pipeline para la tokenizaci贸n y generaci贸n del texto\n",
        "tinyllama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.1,\n",
        "    max_new_tokens=200,\n",
        "    eos_token_id=CHAT_EOS_TOKEN_ID,\n",
        ")"
      ],
      "metadata": {
        "id": "me-YzlbLIev3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"#Person1#: What's wrong with you? Why are you scratching so much?\n",
        "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
        "#Person1#: Let me have a look. Whoa! Get away from me!\n",
        "#Person2#: What's wrong?\n",
        "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
        "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
        "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
        "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\"\"\""
      ],
      "metadata": {
        "id": "Surp4mr4NIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "id": "4lt-Z00kM3xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invocamos el pipeline para realizar generaci贸n de texto\n",
        "output = tinyllama_pipe(prompt_template)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "6Bm6cMn4IpFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}