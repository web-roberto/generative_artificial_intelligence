{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7b12c"
      },
      "source": [
        "# Parameter Efficient Fine-tuning (QLoRA) de LLAMA 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e9ea52"
      },
      "source": [
        "<div style=\"background-color:#D9EEFF;color:black;padding:2%;\">\n",
        "<h2>Enunciado del caso práctico</h2>\n",
        "\n",
        "En este caso práctico, se propone al alumno la realización de Parameter Efficient Fine-tuning (PEFT) sobre uno de los LLMs más potentes y populares de la actualidad, su nombre es [LLAMA 2](https://ai.meta.com/llama/) de la compañía Meta (anteriormente Facebook).\n",
        "\n",
        "Para este ejercicio concreto se propone el uso de [LLAMA-2-7b-chat](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) que es una versión de LLAMA-2 que tiene 7.000 millones de parámetros.\n",
        "\n",
        "Concretamente, se propone un escenario en el que un periódico digital quiere generar automáticamente el título de sus artículos, la descripción y las palabras clave utilizadas para el SEO.\n",
        "\n",
        "Un factor importante es que la generación de esta información no debe ser genérica, debe adaptarse al estilo y tipo de títulos, descripciones y palabras clave que ha generado el periódico para otros artículos pasados.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831d29b1"
      },
      "source": [
        "# Resolución del caso práctico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V8dgd_BUeOK"
      },
      "source": [
        "## 0. Instalación de librerías externas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Peqj0qS88f5v"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNFCLMHWCXs"
      },
      "source": [
        "## 1. Comportamiento de [LLAMA-2-7B-Chat](https://ai.meta.com/llama/) sin Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br7gt6CJwfgy"
      },
      "source": [
        "### 1.1. Lectura del modelo y tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBH5vycvEZ2s"
      },
      "source": [
        "En este ejercicio vamos a cargar el modelo de una manera especial. Concretamente, vamos a aplicar una técnica denominada QLoRA (Quantized LoRA).\n",
        "\n",
        "QLoRA es un enfoque de ajuste eficiente que reduce el uso de memoria lo suficiente como para ajustar un modelo de 65B parámetros en una sola GPU de 48 GB, al tiempo que conserva el rendimiento completo de la tarea de ajuste de 16 bits.\n",
        "\n",
        "Un modelo cuantificado es un modelo que tiene sus parámetros/pesos en un tipo de datos inferior al tipo de datos en el que fue entrenado. Por ejemplo, si se entrena con un tipo de dato float de 32 bits y luego se convierten esos parámetros/pesos a un tipo de datos inferior, como float de 16/8/4 bits, el efecto sobre el rendimiento del modelo es mínimo o nulo.\n",
        "\n",
        "En este caso vamos a reducir la precisión de los parámetros del LLM a 4 bits. Esto ayuda a disminuir enormemente el consumo de memoria del modelo, lo que facilita su manipulación y ajuste (finetuning) en hardware más accesible, como una única GPU.\n",
        "\n",
        "Repositorio GitHub: https://github.com/artidoro/qlora\n",
        "\n",
        "Artículo: https://arxiv.org/abs/2305.14314"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdK3d284lxFw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Definimos los paramétros para bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNfU5_BKOTp1"
      },
      "outputs": [],
      "source": [
        "# Leemos el modelo pre-entrenado el modelo LLAMA2-7b-chat\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"NousResearch/Llama-2-7b-chat-hf\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0},\n",
        "    # low_cpu_mem_usage=True # Reduccion del consumo de cpu y memoria al leer el modelo\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1 # Un valor distinto de 1 activará el cálculo más preciso pero más lento de las capas lineales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAl7HAdNPjDT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652jxcqawjQk"
      },
      "source": [
        "### 1.2. Generación de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ErX-b24n4Ll"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "llama2_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bKdgvUSVYfP"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"<<SYS>>\n",
        "Comportate como un ChatBot amigable experto en programación en Python\n",
        "<</SYS>>\n",
        "\n",
        "Desarrolla un programa en Python que ordene una lista de 10 numeros enteros.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DCvtETXWJJT"
      },
      "outputs": [],
      "source": [
        "prompt = \"Actúa como si fueses el mayor experto en historia del mundo. Describe \\\n",
        "en pocas palabras lo que ocurrió en la segunda guerra mundial.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pelMVjGATwUO"
      },
      "outputs": [],
      "source": [
        "prompt_template = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = llama2_pipe(prompt_template)\n",
        "print(output[0]['generated_text'].replace(\"[/INST]\", \"[/INST]\\n\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfNc7BPy_u9N"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\"La Revolución Industrial, que tuvo lugar principalmente en el siglo XIX, \\\n",
        "fue un período de grandes cambios tecnológicos, culturales y socioeconómicos que \\\n",
        "transformó a las sociedades agrarias en sociedades industriales. Durante este tiempo, \\\n",
        "hubo un cambio masivo de mano de obra de las granjas a las fábricas. Esto se debió a \\\n",
        "la invención de nuevas máquinas que podían realizar tareas más rápido y eficientemente \\\n",
        "que los humanos o los animales. Esta transición llevó a un aumento en la producción de \\\n",
        "bienes, pero también tuvo consecuencias negativas, como la explotación \\laboral y la \\\n",
        "contaminación ambiental.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOaiJBSJ_vr-"
      },
      "outputs": [],
      "source": [
        "prompt_template = f\"<s>[INST] <<SYS>>Dado un artículo de noticias, proporciona \\\n",
        "los siguientes campos en un diccionario JSON: \\'titulo\\', \\'SEO\\' y \\'resumen\\'\\\n",
        "<</SYS>> {prompt} [/INST]\"\n",
        "\n",
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = llama2_pipe(prompt_template)\n",
        "print(output[0]['generated_text'].replace(\"[/INST]\", \"[/INST]\\n\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01t6CnUB_Lns"
      },
      "source": [
        "## 2. Selección y preparación del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS5_dopuSok0"
      },
      "source": [
        "Al igual que para el resto de los LLMs, uno de los puntos más importantes de la selección y preparación de los datos es el formato de los ejemplos de entrenamiento.\n",
        "\n",
        "En el caso de Llama 2, los autores utilizaron la siguiente plantilla para los modelos de chat:\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "System prompt\n",
        "<</SYS>>\n",
        "\n",
        "User prompt [/INST] Model answer </s>\n",
        "```\n",
        "La plantilla se compone de elementos opcionales y obligatorios:\n",
        "* Instrucciones del sistema (opcionales) para guiar al modelo\n",
        "* Instrucciones del usuario (obligatorias) para dar la instrucción\n",
        "* Entradas adicionales (opcionales) a tener en cuenta\n",
        "* Respuesta del modelo (obligatoria)\n",
        "\n",
        "A continuación puede observarse un ejemplo de nuestro conjunto de datos:\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>Dado un artículo de noticias, proporciona los siguientes campos en un diccionario JSON: \\'titulo\\', \\'SEO\\' y \\'resumen\\'<</SYS>>  La empresa pública Canal de Isabel II congelará este 2019 las tarifas de agua por cuarto año consecutivo para los hogares de la región. Además, el Canal ha eliminado las órdenes de corte de suministro por impago a aquellas familias que no puedan abonar el suministro por problemas económicos, y extenderá bonificaciones a los perceptores de pensiones de viudedad con ingresos inferiores a los 14.000 euros brutos anuales. El bono social de Canal se aplica ya a los perceptores de la Renta Mínima de Inserción y de la Renta Activa de Inserción del Servicio Público de Empleo Estatal, pensiones no contributivas y, en general, a todas aquellas personas en situación de vulnerabilidad, además de familias numerosas. La Comunidad de Madrid volverá a congelar las tarifas del transporte público para este año 2019. Según lo recogido en los presupuestos regionales, los precios del abono transporte y los billetes sencillos para Metro, EMT y autobuses interurbanos se mantendrán en los mismos niveles que se fijaron en 2013. Se mantiene también la tarifa plana de 20 euros para el Abono de Transporte Joven, que ya cuenta con 1,3 millones de usuarios. Aparte, para este año se prevé la apertura de la nueva estación de Metro de Arroyo Fresno (L7) para el primer trimestre del año y que dará servicio a 220.000 vecinos de los barrios periféricos del noroeste de la capital. A ello se sumará también la reapertura de la \\\\\"renovada\\\\\" estación de Gran Vía, que servirá de intercambiador con la estación de Cercanías de la estación de Sol. También continuarán los trabajos para ampliar la Línea 11. A nivel estatal, el año 2019 se inicia con subidas en la luz, los carburantes, varios productos de telefonía, los sellos y los billetes regionales, mientras que bajará el gas natural, en una media del 1,92% el IBI y se mantendrán las tarifas de Cercanías y las tasas aeroportuarias [Qué sube y qué baja en 2019]. Sigue con nosotros la actualidad de Madrid en Facebook, en Twitter y en nuestro Patio de Vecinos en Instagram[/INST] \"{\\\\\"titulo\\\\\": \\\\\"Congelación de tarifas de agua y transporte público en Madrid para 2019\\\\\", \\\\\"SEO\\\\\": [\\\\\"Canal de Isabel II, tarifas de agua, transporte público, Madrid, 2019\\\\\"], \\\\\"resumen\\\\\": \\\\\"Canal de Isabel II congela tarifas de agua. Madrid mantiene precios de transporte público y anuncia mejoras en 2019.\\\\\"}\" </s>```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBM_wgklBoJE"
      },
      "source": [
        "### 2.1. Lectura del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY_OMLEPfY73"
      },
      "outputs": [],
      "source": [
        "# Leemos el conjunto de datos de Google Drive\n",
        "dataset_path = \"/content/drive/MyDrive/datasets/llama-2-fine-tuning-datset.txt\"\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "  examples = f.read().splitlines() # De esta forma no sale el \\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSNnz4XzftGW"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "ds = Dataset.from_dict({\"text\": examples})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g1L1_AGgMzo"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMV61GDY9mHK"
      },
      "outputs": [],
      "source": [
        "ds[\"text\"][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVyogGn0ruV"
      },
      "source": [
        "## 3. Fine-tuning del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKOiXMJBMBPD"
      },
      "source": [
        "### 3.1. Configuración de LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnDnMYV-HYtW"
      },
      "source": [
        "La siguiente función es interesante para comparar el número de parámetros entrenables que tiene el modelo antes y después de apalicar LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWyRL5IhUpu3"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ7xJDTFUqtF"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxiTkz4zy8aK"
      },
      "source": [
        "Configuramos los parámetros de LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-il4eI-L4yg"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "\n",
        "# Definición de la configuración de LoRA\n",
        "lora_config = LoraConfig(\n",
        "                 r = 16, # Dimensión de las matrices\n",
        "                 lora_alpha = 16, # LoRA scaling factor\n",
        "                 lora_dropout = 0.05, # Regularización\n",
        "                 bias=\"none\",\n",
        "                 task_type=\"CAUSAL_LM\" # Tipo de tarea/modelo al que aplicarlo\n",
        "                 # target_modules=[\"q\", \"k\", \"v\"], # Los módulos (ej. Attention Heads) donde aplicar las matrices\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtqgSmk7zB5U"
      },
      "outputs": [],
      "source": [
        "# Aplicamos la configuración al modelo\n",
        "model_peft = get_peft_model(model, lora_config)\n",
        "\n",
        "# Mostramos el número de parámetros que se van a entrenar\n",
        "model_peft.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTs5vb_RTQp"
      },
      "source": [
        "### 3.2. Preparación y ejecución del fine-tuning (entrenamiento)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVY45QhS8G4W"
      },
      "source": [
        "### Definición de los parámetros del entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C8yMSK9jMvb"
      },
      "outputs": [],
      "source": [
        "# Directorio de salida donde se almacenarán las predicciones del modelo y los puntos de control\n",
        "output_dir = \"/content/drive/MyDrive/llama2-7b-chat-peft\"\n",
        "\n",
        "# Número de epochs de entrenamiento\n",
        "num_train_epochs = 5\n",
        "\n",
        "# Habilitar entrenamiento fp16/bf16 (establecer bf16 en True con un A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Tamaño del lote por GPU para el entrenamiento\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Tamaño del lote por GPU para la evaluación\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Número de pasos de actualización para acumular los gradientes\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Habilitar el registro de puntos de control de gradientes\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Norma máxima del gradiente (recorte del gradiente)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Tasa de aprendizaje inicial (optimizador AdamW)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Decaimiento de pesos a aplicar a todas las capas excepto a los pesos de sesgo/LayerNorm\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizador a usar\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Programación de la tasa de aprendizaje\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Número de pasos de entrenamiento (anula num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Proporción de pasos para un calentamiento lineal (de 0 a tasa de aprendizaje)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Agrupar secuencias en lotes del mismo tamaño\n",
        "# Ahorra memoria y acelera considerablemente el entrenamiento\n",
        "group_by_length = True\n",
        "\n",
        "# Guardar punto de control cada X pasos de actualización\n",
        "save_steps = 0\n",
        "\n",
        "# Registrar cada X pasos de actualización\n",
        "logging_steps = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCY-cd0K1XM7"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "# Creamos la instancia de entrenamiento\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=ds,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=None, # Cuando es None, el max_seq_len vendrá determinado por la secuencia más larga de un lote\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False, # Empaquetar múltiples ejemplos cortos en la misma secuencia de entrada para aumentar la eficiencia\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYmLeJXAH6k_"
      },
      "source": [
        "### 3.3. Entrenamiento y almacenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmH7IV5U2YNL"
      },
      "outputs": [],
      "source": [
        "# Iniciamos el entrenamiento\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUsi23MF9jxF"
      },
      "outputs": [],
      "source": [
        "# Guardamos el tokenizador en disco para utilizarlo posteriormente\n",
        "tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JF_Kn0SNjdE"
      },
      "source": [
        "## 4. Generación de texto con LLAMA 2 Fine-tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwQTEtYxxJL2"
      },
      "source": [
        "### 4.1. Lectura del modelo y del tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3VY4Kh5ws2j"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "llama2_ft_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKIZYLD34_Yo"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\"La Revolución Industrial, que tuvo lugar principalmente en el siglo XIX, \\\n",
        "fue un período de grandes cambios tecnológicos, culturales y socioeconómicos que \\\n",
        "transformó a las sociedades agrarias en sociedades industriales. Durante este tiempo, \\\n",
        "hubo un cambio masivo de mano de obra de las granjas a las fábricas. Esto se debió a \\\n",
        "la invención de nuevas máquinas que podían realizar tareas más rápido y eficientemente \\\n",
        "que los humanos o los animales. Esta transición llevó a un aumento en la producción de \\\n",
        "bienes, pero también tuvo consecuencias negativas, como la explotación \\laboral y la \\\n",
        "contaminación ambiental.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJqcPQNp4xZA"
      },
      "outputs": [],
      "source": [
        "prompt_template = f\"<s>[INST] <<SYS>>Dado un artículo de noticias, proporciona \\\n",
        "los siguientes campos en un diccionario JSON: \\'titulo\\', \\'SEO\\' y \\'resumen\\'\\\n",
        "<</SYS>> {prompt} [/INST]\"\n",
        "\n",
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = llama2_ft_pipe(prompt_template)\n",
        "print(output[0]['generated_text'].replace(\"[/INST]\", \"[/INST]\\n\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxzxhCOU07h6"
      },
      "source": [
        "### 4.2. Cargar el modelo almacenado en disco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Mq54AG3eQt"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBGVT81_07RG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "adapters_name = \"/content/drive/MyDrive/llama2-7b-chat-peft/checkpoint-15\"\n",
        "\n",
        "print(f\"Cargando el modelo: '{model_name}' en memoria...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapters_name)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\"El modelo: '{model_name}' ha sido cargado correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3rfiuEu3tm-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Leemos el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/llama2-7b-chat-peft/tokenizer\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM61RyJL1t08"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Creamos un pipeline para la tokenización y generación del texto\n",
        "llama2_ft_pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS-ey8oD1tyO"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\"La Revolución Industrial, que tuvo lugar principalmente en el siglo XIX, \\\n",
        "fue un período de grandes cambios tecnológicos, culturales y socioeconómicos que \\\n",
        "transformó a las sociedades agrarias en sociedades industriales. Durante este tiempo, \\\n",
        "hubo un cambio masivo de mano de obra de las granjas a las fábricas. Esto se debió a \\\n",
        "la invención de nuevas máquinas que podían realizar tareas más rápido y eficientemente \\\n",
        "que los humanos o los animales. Esta transición llevó a un aumento en la producción de \\\n",
        "bienes, pero también tuvo consecuencias negativas, como la explotación \\laboral y la \\\n",
        "contaminación ambiental.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6A_qDp11xxV"
      },
      "outputs": [],
      "source": [
        "prompt_template = f\"<s>[INST] <<SYS>>Dado un artículo de noticias, proporciona \\\n",
        "los siguientes campos en un diccionario JSON: \\'titulo\\', \\'SEO\\' y \\'resumen\\'\\\n",
        "<</SYS>> {prompt} [/INST]\"\n",
        "\n",
        "# Invocamos el pipeline para realizar generación de texto\n",
        "output = llama2_ft_pipe(prompt_template)\n",
        "print(output[0]['generated_text'].replace(\"[/INST]\", \"[/INST]\\n\\n\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}